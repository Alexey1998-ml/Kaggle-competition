{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Alexey1998-ml/example/blob/master/catch_me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "APqRITJb9qDF",
    "outputId": "9d66a4f9-450b-4a60-858b-40d79e33c9d5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HXtJ78p4Jka"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,\\\n",
    "learning_curve, TimeSeriesSplit\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fixb8ixl5k00"
   },
   "outputs": [],
   "source": [
    "colab = None\n",
    "if colab:\n",
    "    PATH_TO_TRAIN = 'drive/My Drive/catch_me_test_sessions.csv'\n",
    "    PATH_TO_TEST = 'drive/My Drive/catch_me_train_sessions.csv'\n",
    "else:\n",
    "    PATH_TO_TRAIN = 'catch_me_test_sessions.csv'\n",
    "    PATH_TO_TEST = 'catch_me_train_sessions.csv'\n",
    "PREDICT_FILENAME = 'catch_me_preds.csv'\n",
    "test = pd.read_csv(PATH_TO_TRAIN)\n",
    "data = pd.read_csv(PATH_TO_TEST)\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "sites = ['site%s' % i for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc (model, data, predict = True, target = 'target'):\n",
    "    batch = trainer.make_batch(data, predict = predict)\n",
    "    roc_auc = roc_auc_score(data[target],\n",
    "                            model(torch.tensor(batch['sites'], dtype=torch.long).cuda(),\n",
    "                                 torch.tensor(batch['cat'], dtype=torch.long).float().cuda()).cpu().data.numpy())\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIk-VMGt6GyX"
   },
   "outputs": [],
   "source": [
    "class Prepare:\n",
    "    def __init__(self, train, test, target_column : str = 'target', **kwargs):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.target_column = target_column\n",
    "        self.target = train[target_column]\n",
    "        self.unknown_idx = -1\n",
    "        self.pad_idx = 0\n",
    "        self.test_size = 0.15\n",
    "        self.batch_size = 1024\n",
    "        self.dropout = 0.3\n",
    "\n",
    "    def prepare_df (self):\n",
    "        train = self.train\n",
    "        test = self.test\n",
    "        train[times] = train[times].apply(pd.to_datetime)\n",
    "        test[times] = test[times].apply(pd.to_datetime)\n",
    "        train[sites] = train[sites].fillna(self.pad_idx).astype(int)\n",
    "        train = train.sort_values('time1')\n",
    "        train = self._create_cat_features(train)\n",
    "        test = self._create_cat_features(test)\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.sites_to_id = False\n",
    "    \n",
    "    def _create_cat_features(self, df):\n",
    "        cat = 'cat_'\n",
    "        df[f'{cat}hour'] = df['time1'].apply(lambda time: time.hour)\n",
    "        df[f'{cat}morning'] = ((df[f'{cat}hour'] >= 7) & (df[f'{cat}hour'] <= 11)).astype('int')\n",
    "        df[f'{cat}day'] = ((df[f'{cat}hour'] >= 12) & (df[f'{cat}hour'] <= 18)).astype('int')\n",
    "        df[f'{cat}evening'] = ((df[f'{cat}hour'] >= 19) & (df[f'{cat}hour'] <= 23)).astype('int')\n",
    "        df[f'{cat}day_of_week'] = df['time1'].apply(lambda t: t.weekday()).astype('int')\n",
    "        df[f'{cat}day_of_week'] = df['time1'].apply(lambda t: t.weekday()).astype('int')\n",
    "        self.cat_columns = list(filter(lambda col: col[:4] == 'cat_', df.columns))\n",
    "        return df\n",
    "\n",
    "    def _get_known_sites(self, train, number_sites = 5000):\n",
    "        counter = Counter()\n",
    "        alice_sites = Counter()\n",
    "        for site in sites:\n",
    "            for num_site in train[site]:\n",
    "                counter.update({num_site})\n",
    "        for site in sites:\n",
    "            for num_site in train[train['target'] == 1][site]:\n",
    "                alice_sites.update({num_site})\n",
    "        known_sites = [site for site, counts in dict(counter.most_common(number_sites)).items()]\n",
    "        known_sites += [site for site, counts in alice_sites.items()]\n",
    "        known_sites += [self.pad_idx, self.unknown_idx]\n",
    "        known_sites = set(known_sites)\n",
    "        sites_to_id = {site : inx for inx, site in enumerate(known_sites)}\n",
    "        self.id_unknown_idx = sites_to_id[self.unknown_idx]\n",
    "        self.sites_to_id = sites_to_id\n",
    "        return sites_to_id\n",
    "\n",
    "    def get_df_for_train(self):\n",
    "        train_df, valid_df, train_target, valid_target = train_test_split(self.train, self.target, random_state = 42,\n",
    "                                                                          shuffle = False, test_size = self.test_size)\n",
    "        sites_to_id = self._get_known_sites(train_df)\n",
    "        self.sites_to_id = sites_to_id\n",
    "        sites_to_id = self._get_known_sites(train_df)\n",
    "        train_df[sites] = train_df[sites].applymap(lambda num_site: sites_to_id[num_site] if num_site in sites_to_id \n",
    "                                                   else sites_to_id[self.unknown_idx])\n",
    "        valid_df[sites] = valid_df[sites].applymap(lambda num_site: sites_to_id[num_site] if num_site in sites_to_id \n",
    "                                                   else sites_to_id[self.unknown_idx])\n",
    "        return train_df, valid_df\n",
    "    \n",
    "    def _get_df_for_test(self):\n",
    "        test = self.test\n",
    "        sites_to_id = self.sites_to_id\n",
    "        test[sites] = test[sites].applymap(lambda num_site: sites_to_id[num_site] if num_site in sites_to_id \n",
    "                                           else sites_to_id[self.unknown_idx])\n",
    "        self.test = test\n",
    "        return test\n",
    "    \n",
    "    def get_train_df(self):\n",
    "        train = self.train\n",
    "        sites_to_id = self._get_known_sites(train)\n",
    "        self.sites_to_id = sites_to_id\n",
    "        train[sites] = train[sites].applymap(lambda num_site: sites_to_id[num_site] if num_site in sites_to_id \n",
    "                                           else sites_to_id[self.unknown_idx])\n",
    "        self.train = train\n",
    "        return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7syXo7AsyFpM"
   },
   "outputs": [],
   "source": [
    "class TrainNN(Prepare):\n",
    "    \n",
    "    def make_batch (self, df, dropout = 0, predict = False, **kwargs):\n",
    "        if 'is_test' in kwargs:\n",
    "            is_test = kwargs['is_test']\n",
    "        else:\n",
    "            is_test = False\n",
    "        if self.dropout:\n",
    "            dropout = self.dropout\n",
    "            \n",
    "        batch = {}\n",
    "        batch['sites'] = df[sites].values\n",
    "        if dropout > 0 and not predict:\n",
    "            batch['sites'] = self._apply_word_dropout(batch['sites'], dropout, self.id_unknown_idx)\n",
    "            \n",
    "        if not is_test:\n",
    "            batch[self.target_column] = df[self.target_column].values\n",
    "        batch['cat'] = df[self.cat_columns].values\n",
    "        return batch\n",
    "\n",
    "    def _apply_word_dropout (self, matrix, keep_prop, replace_with = -1, pad_ix = 0):\n",
    "        dropout_mask = np.random.choice(2, matrix.shape, p = [1 - keep_prop, keep_prop])\n",
    "        dropout_mask &= matrix !=  pad_ix\n",
    "        return np.choose (dropout_mask, [matrix, np.full_like(matrix, replace_with)])\n",
    "    \n",
    "    def iterable_minibatches (self, data, shuffle = True, **kwargs):\n",
    "        indecses = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indecses = np.random.permutation(indecses)\n",
    "        for start in range(0, len(indecses), self.batch_size):\n",
    "            batch = self.make_batch(data.iloc[indecses[start : start + self.batch_size]], **kwargs)\n",
    "            target = batch.pop(self.target_column)\n",
    "            yield batch, target\n",
    "    \n",
    "    def make_predictions_file (self, model):\n",
    "        test = self._get_df_for_test()\n",
    "        batch = self.make_batch(test, predict = True, is_test = True)\n",
    "        input_site = torch.tensor(batch['sites'], dtype=torch.long).cuda()\n",
    "        input_cat = torch.tensor(batch['cat'], dtype=torch.long).float().cuda()\n",
    "        predictions = model(input_site, input_cat).cpu().data.numpy()\n",
    "        series_preds = pd.Series(predictions[:,0], index = range (1,predictions[:,0].shape[0] + 1),\n",
    "                  name = 'target')\n",
    "        series_preds.to_csv(PREDICT_FILENAME, header = True, index_label = 'session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjsxINZHufkH"
   },
   "outputs": [],
   "source": [
    "trainer = TrainNN(data, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9dNFt6ay1aC"
   },
   "outputs": [],
   "source": [
    "trainer.prepare_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7Snyg7szDVJ"
   },
   "outputs": [],
   "source": [
    "train_df, valid_df = trainer.get_df_for_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = trainer.get_train_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wz7apoKXyMLP"
   },
   "outputs": [],
   "source": [
    "class Reorder(nn.Module):\n",
    "    def forward (self, input):\n",
    "        return input.permute(0, 2, 1)\n",
    "\n",
    "class Flatten (nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcVmov9A6NtE"
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, n_tokens = len(trainer.sites_to_id), hid_size = 64, \n",
    "                   len_sequence = 10, len_cat = len(trainer.cat_columns)):\n",
    "        super(NN, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, hid_size)\n",
    "        self.reorder = Reorder()\n",
    "        self.cnn1 = nn.Conv1d(in_channels = hid_size,\n",
    "                             out_channels = hid_size,\n",
    "                             kernel_size = 3)\n",
    "        self.cnn2 = nn.Conv1d(in_channels = hid_size,\n",
    "                             out_channels = hid_size,\n",
    "                             kernel_size = 2)\n",
    "        self.cnn3 = nn.Conv1d(in_channels = hid_size,\n",
    "                             out_channels = hid_size,\n",
    "                             kernel_size = 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(3*hid_size + len_cat, hid_size)\n",
    "        self.fc2 = nn.Linear(hid_size, 1)\n",
    "        self.fc_cat = nn.Linear(len_cat, len_cat)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.batchnorm = nn.BatchNorm1d(3*hid_size + len_cat)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, input_site, input_cat):\n",
    "        emb = self.emb(input_site)\n",
    "        emb = self.reorder(emb)\n",
    "        cnn1 = self.relu(self.cnn1(emb))\n",
    "        cnn2 = self.relu(self.cnn2(emb))\n",
    "        cnn3 = self.relu(self.cnn3(emb))\n",
    "        pool1 = self.maxpool(cnn1)\n",
    "        pool2 = self.maxpool(cnn2)\n",
    "        pool3 = self.maxpool(cnn3)\n",
    "        flatten1 = self.flatten(pool1)\n",
    "        flatten2 = self.flatten(pool2)\n",
    "        flatten3 = self.flatten(pool3)\n",
    "        cat = self.fc_cat(input_cat)\n",
    "        flatten = torch.cat([flatten1.view(flatten1.size(0), -1),\n",
    "                             flatten2.view(flatten2.size(0), -1),\n",
    "                             flatten3.view(flatten3.size(0), -1),\n",
    "                             cat.view(cat.size(0), -1)], dim = 1)\n",
    "        flatten = self.batchnorm(flatten)\n",
    "        flatten = self.dropout(flatten)\n",
    "        out = self.fc1(flatten)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "XKg_38X568AF",
    "outputId": "101d3516-e363-4aba-8e7c-472f36524844"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (emb): Embedding(5415, 64)\n",
       "  (reorder): Reorder()\n",
       "  (cnn1): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
       "  (cnn2): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n",
       "  (cnn3): Conv1d(64, 64, kernel_size=(4,), stride=(1,))\n",
       "  (relu): ReLU()\n",
       "  (maxpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (flatten): Flatten()\n",
       "  (fc1): Linear(in_features=197, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (fc_cat): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (batchnorm): BatchNorm1d(197, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN()\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGlbDX6h-jqt"
   },
   "outputs": [],
   "source": [
    "batch, target = next(trainer.iterable_minibatches(train_df))\n",
    "input_site = torch.tensor(batch['sites'], dtype=torch.long).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "colab_type": "code",
    "id": "8O3YSw9A7N3b",
    "outputId": "da67a444-231d-400e-cb32-d2d82ccced1a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c070fab029b940d5ad98084dc05509e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 620.00 MiB (GPU 0; 2.00 GiB total capacity; 651.01 MiB already allocated; 603.89 MiB free; 664.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-96bef6713b93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mhistory_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#         history_val.append(roc_auc(model, valid_df))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mhistory_train_roc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-27b2459cd534>\u001b[0m in \u001b[0;36mroc_auc\u001b[1;34m(model, data, predict, target)\u001b[0m\n\u001b[0;32m      3\u001b[0m     roc_auc = roc_auc_score(data[target],\n\u001b[0;32m      4\u001b[0m                             model(torch.tensor(batch['sites'], dtype=torch.long).cuda(),\n\u001b[1;32m----> 5\u001b[1;33m                                  torch.tensor(batch['cat'], dtype=torch.long).float().cuda()).cpu().data.numpy())\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-d0a8f006cf8c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_site, input_cat)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_site\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mcnn1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mcnn2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mcnn3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 202\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 620.00 MiB (GPU 0; 2.00 GiB total capacity; 651.01 MiB already allocated; 603.89 MiB free; 664.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "history_train = []\n",
    "history_val = []\n",
    "history_train_roc = []\n",
    "epochs = 300\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss_function = nn.BCELoss().to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for idx, (batch, target) in tqdm_notebook(enumerate(trainer.iterable_minibatches(train))):\n",
    "        input_site = torch.tensor(batch['sites'], dtype=torch.long).cuda()\n",
    "        input_cat = torch.tensor(batch['cat'], dtype=torch.long).float().cuda()\n",
    "        target = torch.tensor(target).float().to(device)\n",
    "\n",
    "        predictions = model(input_site, input_cat).float()\n",
    "        predictions = predictions.view(predictions.size(0))\n",
    "        loss = loss_function(predictions, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "\n",
    "        history_train.append(loss.data.cpu().numpy())\n",
    "#         history_val.append(roc_auc(model, valid_df))\n",
    "        history_train_roc.append(roc_auc(model, train))\n",
    "        if (idx+1)%50==0:\n",
    "            clear_output(True)\n",
    "            # plt.plot(history_train,label='train loss')\n",
    "#             plt.plot(history_val,label='val')\n",
    "            plt.plot(history_train_roc, label = 'train')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "#             print(f'val {history_val[-1]}')\n",
    "            print(f'train {history_val[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.make_predictions_file(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM9b90ETe5gOdBrOd9mhpam",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1q7pI8KwTPJ3fvsxzWbr_O5k-A8TRlvBl",
   "name": "catch_me.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
